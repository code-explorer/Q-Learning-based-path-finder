{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants declared for different penalties and rewards.  \n",
    "NOTE: Rewards greater than or equal to 10000 can cause the q-table to overflow. So, the gold reward has been taken as 1000 rather than 10000 to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10 # size of matrix\n",
    "MOVE_PENALTY = 1 # move penalty of every move made\n",
    "METAL_DETECTOR_PENALTY = 100 # penalty of getting in range of a metal detector\n",
    "GOLD_REWARD = 1000 # reward for reaching the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the the robot, gold and the metal detectors are mapped from the given question.  \n",
    "The coordinate systems origin is taken as the upper-left most space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_coordinate = ((0, 9))\n",
    "metal_detector_coordinates = ((1, 4), (2, 9), (3, 2), (3, 5), (7, 3), (7, 8), (8, 1))\n",
    "gold_coordinate = ((9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is desinged to calculate the surroundings of the metal detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surrounding_coordinates(input_coordinates):\n",
    "    surrounding_coordinates = []\n",
    "    for coordinate in input_coordinates:\n",
    "        for x_offset in range(-1, 2):\n",
    "            for y_offset in range(-1, 2):\n",
    "                surrounding_coordinates.append((coordinate[0]+x_offset, coordinate[1]+y_offset))\n",
    "    return surrounding_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A robot class is being defined in a similar way to the openAI's gym environment.  \n",
    "Whenever the robot wants to make a move, the step function is initiated with the action that the robot wants to take as the parameter.  \n",
    "The actions are as follows  \n",
    "step(0) - go right  \n",
    "step(1) - go left  \n",
    "step(2) - go down  \n",
    "step(3) - go up  \n",
    "If the robot tries to move out of the map, it stays in the same positions but gets a negative reward of -1 for taking a step.  \n",
    "When the step function is initiated, it returns 3 values:  \n",
    "1. Updated robots position  \n",
    "2. Positive/Negative reward based on the location it is in  \n",
    "3. Whether the robot has reached the goal or not  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self):\n",
    "        self.x, self.y = robot_coordinate\n",
    "\n",
    "    def step(self, choice):\n",
    "        if choice == 0:\n",
    "            return self.move(1, 0)\n",
    "        elif choice == 1:\n",
    "            return self.move(-1, 0)\n",
    "        elif choice == 2:\n",
    "            return self.move(0, 1)\n",
    "        elif choice == 3:\n",
    "            return self.move(0, -1)\n",
    "\n",
    "    def move(self, x=0, y=0):\n",
    "        self.x += x\n",
    "        self.y += y\n",
    "\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > SIZE - 1:\n",
    "            self.x = SIZE - 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > SIZE - 1:\n",
    "            self.y = SIZE - 1\n",
    "\n",
    "        return (self.x, self.y), self.calculate_reward(), self.done()\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        if (self.x, self.y) == gold_coordinate:\n",
    "            return GOLD_REWARD - MOVE_PENALTY\n",
    "        elif (self.x, self.y) in calculate_surrounding_coordinates(metal_detector_coordinates):\n",
    "            return - METAL_DETECTOR_PENALTY - MOVE_PENALTY\n",
    "        else:\n",
    "            return - MOVE_PENALTY\n",
    "\n",
    "    def done(self):\n",
    "        if (self.x, self.y) == gold_coordinate:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x, self.y = robot_coordinate\n",
    "        return self.x, self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Robot() # constructor for creating the robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function which return the q-table after the agent has gone through a given number of episodes with the given parameters.\n",
    "The following parameters are passed to the function in addition to the robot object and the number of episodes the robot should train for:\n",
    "1. Discounting factor - y - this decreases the impact of future rewards on the immediate decision making - the greater the discount factor, the more the robot tries to get immediate rewards.\n",
    "2. Epsilon - eps - Allows the robot the explore more\n",
    "3. Learning rate - lr - Q-table learning rate\n",
    "4. Decay factor - decay_factor - Rate of exploration is reduced by this factor\n",
    "  \n",
    "parameters = (y = 0.95, eps = 0.5, lr = 0.8, decay_factor = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500, parameters = (0.95, 0.5, 0.8, 0.999)):\n",
    "    q_table = np.zeros((4, 10, 10), dtype= np.int64)\n",
    "    y, eps, lr, decay_factor = parameters\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[:, s[1], s[0]]) == 0:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = np.argmax(q_table[:, s[1], s[0]])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done = env.step(a)\n",
    "            q_table[a][s[1]][s[0]] += r + lr * (y * np.max(q_table[:, new_s[1], new_s[0]]) - q_table[a][s[1]][s[0]])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to display the path that the robot takes using the q-table. Reterns the path as well as whether the robot was able to reach the goal or not.  \n",
    "If the robot tries to go in an infinite loop, then that path is not accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_path_from_q_table(q_table):\n",
    "    coordinate = list(robot_coordinate)\n",
    "    path = np.zeros((10, 10), dtype= np.int8)\n",
    "    while coordinate != list(gold_coordinate):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        \n",
    "        if path[coordinate[1]][coordinate[0]] != 99:\n",
    "            path[coordinate[1]][coordinate[0]] = 99\n",
    "        else:\n",
    "            # to prevent an infinite loop, the code breaks when the program tries to go into an infinite loop\n",
    "            return path, False\n",
    "            \n",
    "        a = np.argmax(q_table[:, coordinate[1], coordinate[0]])\n",
    "\n",
    "        if a == 0:\n",
    "            y = 1\n",
    "        elif a == 1:\n",
    "            y = -1\n",
    "        elif a == 2:\n",
    "            x = 1\n",
    "        elif a == 3:\n",
    "            x = -1\n",
    "    \n",
    "        coordinate[1] += x\n",
    "        coordinate[0] += y\n",
    "        \n",
    "        if coordinate[1] < 0:\n",
    "            coordinate[1] = 0\n",
    "        elif coordinate[1] > SIZE - 1:\n",
    "            coordinate[1] = SIZE - 1\n",
    "        if coordinate[0]< 0:\n",
    "            coordinate[0] = 0\n",
    "        elif coordinate[0] > SIZE - 1:\n",
    "            coordinate[0] = SIZE - 1\n",
    "    \n",
    "    for a in metal_detector_coordinates:\n",
    "        path[a[1]][a[0]] = 22\n",
    "    \n",
    "    path[gold_coordinate[1]][gold_coordinate[0]] = 77\n",
    "    return path, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter sets for finding which sets of parameters are the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter_set = (y = 0.95, eps = 0.5, lr = 0.8, decay_factor = 0.999)\n",
    "parameter_set = (0.95, 0.5, 0.8, 0.999)\n",
    "parameter_set_1 = (0.85, 0.5, 0.8, 0.999)\n",
    "parameter_set_2 = (0.95, 0.1, 0.8, 0.999)\n",
    "parameter_set_3 = (0.95, 0.9, 0.8, 0.999)\n",
    "parameter_set_4 = (0.95, 0.5, 0.3, 0.999)\n",
    "parameter_set_5 = (0.95, 0.5, 0.8, 0.699)\n",
    "parameter_sets = (parameter_set_1, parameter_set_2, parameter_set_3, parameter_set_4, parameter_set_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes the above parameters and returns the following:\n",
    "1. The fastest solution, number of iterations it took to find the fastest solution and the parameter set which gave the fastest solution. The fastest path refers to the path which the algorithm was able to find with the least amount of iterations.\n",
    "2. The best solution, number of iterations it took to find the best solution and the parameter set which gave the best solution. The best path refers to the path which the algorithm was able to find which will give the maximum possible rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_set_of_parameters(parameter_sets):\n",
    "    solution_found = False\n",
    "    best_solution_found = False\n",
    "    SOLUTION_FOUND = False\n",
    "    i = 0\n",
    "    q_table = eps_greedy_q_learning_with_table(env, 250, parameter_set)\n",
    "    ACTUAL_BEST_SOLUTION, _ = display_path_from_q_table(q_table)\n",
    "    while not solution_found or not best_solution_found:\n",
    "        i += 1\n",
    "        for p in parameter_sets:\n",
    "            q_table = eps_greedy_q_learning_with_table(env, i, p)\n",
    "            path, solution_found = display_path_from_q_table(q_table)\n",
    "            if solution_found and not SOLUTION_FOUND:\n",
    "                solution, fastestIteration, fastestSet = (path, i, p)\n",
    "                SOLUTION_FOUND = True\n",
    "            if np.array_equal(path, ACTUAL_BEST_SOLUTION):\n",
    "                best_solution_found = True\n",
    "                best_solution, bestIteration, bestSet = (path, i, p)\n",
    "                break\n",
    "    \n",
    "    return (solution, fastestIteration, fastestSet), (best_solution, bestIteration, bestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below line will give the fastest and the best possible paths.  \n",
    "The tiles in which the robot goes through is represented by 99.  \n",
    "The tiles in which the metal detectors are present are represented by 22.  \n",
    "The tile in which the goal is present is represented by 77.  \n",
    "  \n",
    "NOTE: If the path goes through a metal detector, the tile remains as 22 so that it is visible that there is a metal detector there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 22,  0],\n",
       "         [ 0,  0,  0, 22,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0, 22,  0,  0],\n",
       "         [ 0, 22,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, 22,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [99, 99, 99, 99, 99,  0,  0,  0,  0,  0],\n",
       "         [99,  0,  0,  0, 99, 99, 99, 22, 99, 99],\n",
       "         [99,  0, 22,  0,  0,  0,  0,  0,  0, 77]], dtype=int8),\n",
       "  14,\n",
       "  (0.95, 0.9, 0.8, 0.999)),\n",
       " (array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 22,  0],\n",
       "         [ 0,  0,  0, 22,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0, 22,  0,  0],\n",
       "         [ 0, 22,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, 22,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0, 99, 99, 99, 99, 99],\n",
       "         [99, 99, 99, 99, 99, 99,  0,  0,  0, 99],\n",
       "         [99,  0,  0,  0,  0,  0,  0, 22,  0, 99],\n",
       "         [99,  0, 22,  0,  0,  0,  0,  0,  0, 77]], dtype=int8),\n",
       "  15,\n",
       "  (0.95, 0.5, 0.8, 0.699)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_set_of_parameters(parameter_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
